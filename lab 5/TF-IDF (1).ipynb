{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01-TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will here compute the TF-IDF on a corpus of newspaper headlines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by importing needed libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed libraries\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the data into the file *headlines.csv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load the dataset\n",
    "df=pd.read_csv('headlines.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, check the dataset basic information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1999 entries, 0 to 1998\n",
      "Data columns (total 2 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   publish_date   1999 non-null   int64 \n",
      " 1   headline_text  1999 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 31.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# TODO: Have a look at the data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now perform preprocessing on this text data: tokenization, punctuation and stop words removal and stemming.\n",
    "\n",
    "Hint: to do so, use NLTK, *pandas*'s method *apply*, lambda functions and list comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20170721</td>\n",
       "      <td>algorithms can make decisions on behalf of fed...</td>\n",
       "      <td>[algorithm, make, decis, behalf, feder, minist]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20170721</td>\n",
       "      <td>andrew forrests fmg to appeal pilbara native t...</td>\n",
       "      <td>[andrew, forrest, fmg, appeal, pilbara, nativ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20170721</td>\n",
       "      <td>a rural mural in thallan</td>\n",
       "      <td>[rural, mural, thallan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20170721</td>\n",
       "      <td>australia church risks becoming haven for abusers</td>\n",
       "      <td>[australia, church, risk, becom, abus]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20170721</td>\n",
       "      <td>australian company usgfx embroiled in shanghai...</td>\n",
       "      <td>[australian, compani, usgfx, embroil, shanghai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publish_date                                      headline_text  \\\n",
       "0      20170721  algorithms can make decisions on behalf of fed...   \n",
       "1      20170721  andrew forrests fmg to appeal pilbara native t...   \n",
       "2      20170721                           a rural mural in thallan   \n",
       "3      20170721  australia church risks becoming haven for abusers   \n",
       "4      20170721  australian company usgfx embroiled in shanghai...   \n",
       "\n",
       "                                           tokenized  \n",
       "0    [algorithm, make, decis, behalf, feder, minist]  \n",
       "1  [andrew, forrest, fmg, appeal, pilbara, nativ,...  \n",
       "2                            [rural, mural, thallan]  \n",
       "3             [australia, church, risk, becom, abus]  \n",
       "4  [australian, compani, usgfx, embroil, shanghai...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Perform preprocessing\n",
    "# import needed modules\n",
    "import string\n",
    "# Tokenize\n",
    "df['tokenized'] = df['headline_text'].apply(nltk.word_tokenize)\n",
    "# Remove punctuation\n",
    "df['tokenized']=df['tokenized'].apply(lambda tokens: [token for token in tokens if token not in string.punctuation])\n",
    "# Remove stop words\n",
    "df['tokenized']=df['tokenized'].apply(lambda tokens: [token for token in tokens if token.lower() not in nltk.corpus.stopwords.words('english')])\n",
    "# Stem\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "df['tokenized']=df['tokenized'].apply(lambda tokens: [stemmer.stem(token) for token in tokens])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute now the Bag of Words for our data, using scikit-learn.\n",
    "\n",
    "Warning: since we used our own preprocessing, you have to bypass analyzer with identity function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1999, 4271)\n",
      "  (0, 182)\t1\n",
      "  (0, 2321)\t1\n",
      "  (0, 1087)\t1\n",
      "  (0, 447)\t1\n",
      "  (0, 1452)\t1\n",
      "  (0, 2457)\t1\n",
      "  (1, 222)\t1\n",
      "  (1, 1541)\t1\n",
      "  (1, 1514)\t1\n",
      "  (1, 251)\t1\n",
      "  (1, 2832)\t1\n",
      "  (1, 2562)\t1\n",
      "  (1, 3860)\t1\n",
      "  (1, 3281)\t1\n",
      "  (2, 3286)\t1\n",
      "  (2, 2532)\t1\n",
      "  (2, 3802)\t1\n",
      "  (3, 341)\t1\n",
      "  (3, 800)\t1\n",
      "  (3, 3227)\t1\n",
      "  (3, 438)\t1\n",
      "  (3, 103)\t1\n",
      "  (4, 343)\t1\n",
      "  (4, 884)\t1\n",
      "  (4, 4031)\t1\n",
      "  :\t:\n",
      "  (1995, 1056)\t1\n",
      "  (1995, 2643)\t1\n",
      "  (1995, 2479)\t1\n",
      "  (1995, 1012)\t1\n",
      "  (1996, 341)\t1\n",
      "  (1996, 787)\t1\n",
      "  (1996, 3328)\t1\n",
      "  (1996, 3542)\t1\n",
      "  (1996, 3355)\t1\n",
      "  (1996, 1489)\t1\n",
      "  (1996, 2542)\t1\n",
      "  (1996, 1069)\t1\n",
      "  (1996, 2809)\t1\n",
      "  (1997, 341)\t1\n",
      "  (1997, 2955)\t1\n",
      "  (1997, 1411)\t1\n",
      "  (1997, 755)\t1\n",
      "  (1997, 702)\t1\n",
      "  (1997, 1235)\t1\n",
      "  (1997, 3264)\t1\n",
      "  (1998, 884)\t1\n",
      "  (1998, 1241)\t1\n",
      "  (1998, 2937)\t1\n",
      "  (1998, 1826)\t1\n",
      "  (1998, 999)\t1\n"
     ]
    }
   ],
   "source": [
    "# TODO: Compute the BOW of the preprocessed data\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def identity_analyzer(tokens):\n",
    "    return tokens\n",
    "vectorizer=CountVectorizer(analyzer=identity_analyzer)\n",
    "output=vectorizer.fit_transform(df['tokenized'])\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the shape of the BOW, the expected value is `(1999, 4165)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute the Term Frequency and then the Inverse Document Frequency, and check the values are not only zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Compute the TF using the BOW\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidf_matrix = tfidf_transformer.fit_transform(output)\n",
    "tf_matrix = tfidf_matrix.toarray()\n",
    "np.any(tf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.90775528 7.90775528 6.65499231 ... 7.90775528 7.90775528 7.90775528]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Compute the IDF\n",
    "print(tfidf_transformer.idf_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute finally the TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2457)\t0.37940445994614136\n",
      "  (0, 2321)\t0.3267905927279272\n",
      "  (0, 1452)\t0.36465387895025764\n",
      "  (0, 1087)\t0.3852340690707783\n",
      "  (0, 447)\t0.4836746969147799\n",
      "  (0, 182)\t0.4836746969147799\n",
      "  (1, 3860)\t0.3229796671605963\n",
      "  (1, 3281)\t0.327942300480771\n",
      "  (1, 2832)\t0.39063104545637256\n",
      "  (1, 2562)\t0.34651373707753075\n",
      "  (1, 1541)\t0.3756519462531484\n",
      "  (1, 1514)\t0.3756519462531484\n",
      "  (1, 251)\t0.327942300480771\n",
      "  (1, 222)\t0.3545400916768914\n",
      "  (2, 3802)\t0.6136272674992542\n",
      "  (2, 3286)\t0.4969136274673873\n",
      "  (2, 2532)\t0.6136272674992542\n",
      "  (3, 3227)\t0.4412675558631679\n",
      "  (3, 800)\t0.5235336535355645\n",
      "  (3, 438)\t0.48833892208322827\n",
      "  (3, 341)\t0.3197580743141225\n",
      "  (3, 103)\t0.4364347933514976\n",
      "  (4, 4031)\t0.4228385123497916\n",
      "  (4, 3603)\t0.4228385123497916\n",
      "  (4, 3594)\t0.3424134326053298\n",
      "  :\t:\n",
      "  (1995, 2643)\t0.4302755758469799\n",
      "  (1995, 2479)\t0.5485265217155414\n",
      "  (1995, 1056)\t0.4616278141304413\n",
      "  (1995, 1012)\t0.5485265217155414\n",
      "  (1996, 3542)\t0.28621722386943427\n",
      "  (1996, 3355)\t0.3298771294058291\n",
      "  (1996, 3328)\t0.24055343185829647\n",
      "  (1996, 2809)\t0.4205359647251682\n",
      "  (1996, 2542)\t0.3718074452777344\n",
      "  (1996, 1489)\t0.3989732513653019\n",
      "  (1996, 1069)\t0.3718074452777344\n",
      "  (1996, 787)\t0.29548937093004335\n",
      "  (1996, 341)\t0.2270884248123804\n",
      "  (1997, 3264)\t0.4661577740693628\n",
      "  (1997, 2955)\t0.37128230457207045\n",
      "  (1997, 1411)\t0.340002064396846\n",
      "  (1997, 1235)\t0.4252971334206717\n",
      "  (1997, 755)\t0.37749324835081804\n",
      "  (1997, 702)\t0.37749324835081804\n",
      "  (1997, 341)\t0.25172409379216565\n",
      "  (1998, 2937)\t0.38959152210216524\n",
      "  (1998, 1826)\t0.5118313515205288\n",
      "  (1998, 1241)\t0.41725370918779786\n",
      "  (1998, 999)\t0.47698111733223886\n",
      "  (1998, 884)\t0.4296922961658734\n"
     ]
    }
   ],
   "source": [
    "# TODO: compute the TF-IDF\n",
    "print(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the 10 words with the highest and lowest TF-IDF on average?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest 10 words:\n",
      "australia 0.009983014998891407\n",
      "australian 0.009697330148661608\n",
      "new 0.008703107457097205\n",
      "polic 0.00773605920474811\n",
      "say 0.0075404597577821775\n",
      "trump 0.006840891998202151\n",
      "man 0.006548453421337382\n",
      "wa 0.006274671593818187\n",
      "charg 0.006028832916829901\n",
      "sydney 0.005642415973209536\n",
      "Lowest 10 words:\n",
      "fabio 0.0001613676677950104\n",
      "adel 0.0001527054029533165\n",
      "coll 0.0001527054029533165\n",
      "gcfc 0.0001527054029533165\n",
      "geel 0.0001527054029533165\n",
      "gw 0.0001527054029533165\n",
      "haw 0.0001527054029533165\n",
      "melb 0.0001527054029533165\n",
      "nmfc 0.0001527054029533165\n",
      "syd 0.0001527054029533165\n"
     ]
    }
   ],
   "source": [
    "# TODO: Print the 10 words with the highest and lowest TF-IDF on average\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "average_tfidf_scores = tfidf_matrix.mean(axis=0).A1\n",
    "\n",
    "word_tfidf_scores = dict(zip(feature_names, average_tfidf_scores))\n",
    "\n",
    "sorted_words = sorted(word_tfidf_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "top_10_words = sorted_words[:10]\n",
    "\n",
    "bottom_10_words = sorted_words[-10:]\n",
    "\n",
    "print(\"Highest 10 words:\")\n",
    "for word, score in top_10_words:\n",
    "    print(word,score)\n",
    "\n",
    "print(\"Lowest 10 words:\")\n",
    "for word, score in bottom_10_words:\n",
    "    print(word,score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the TF-IDF using scikit-learn on our preprocessed data (the one you used to compute the BOW)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute the TF-IDF using scikit learn\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the 10 highest and lowest TF-IDF words on average to the ones you had by yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lowest words: coll     0.305258\n",
      "gw       0.305258\n",
      "nmfc     0.305258\n",
      "adel     0.305258\n",
      "melb     0.305258\n",
      "syd      0.305258\n",
      "haw      0.305258\n",
      "geel     0.305258\n",
      "gcfc     0.305258\n",
      "fabio    0.322574\n",
      "dtype: float64\n",
      "highest words: mosul        0.779137\n",
      "rig          0.786813\n",
      "travel       0.788050\n",
      "aquapon      0.794899\n",
      "date         0.794899\n",
      "employ       0.795060\n",
      "financ       0.803629\n",
      "mongolian    0.831769\n",
      "pump         1.000000\n",
      "peacemak     1.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# TODO: Print the 10 words with the highest and lowest TF-IDF on average\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you have the same words? How do you explain it?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-panel-2023.05-py310",
   "language": "python",
   "name": "conda-env-anaconda-panel-2023.05-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
